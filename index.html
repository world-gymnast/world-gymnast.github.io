<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="World-Gymnast trains robot policies with reinforcement learning inside a learned world model, using VLM-based rewards.">
  <meta name="keywords" content="robot learning, reinforcement learning, world models, VLA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>World-Gymnast: Training Robots with Reinforcement Learning in a World Model</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/earth-svgrepo-com.svg">
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">World-Gymnast: Training Robots with Reinforcement Learning in a World Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://www.linkedin.com/in/ansh-ks/">Ansh Kumar Sharma</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://www.linkedin.com/in/yixiang-sun-79175324b/">Yixiang Sun</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://www.linkedin.com/in/ninghaolu/">Ninghao Lu</a><sup>1</sup>,</span>
            <span class="author-block">Yunzhe Zhang<sup>1</sup>,</span>
            <span class="author-block">Jiarao Liu<sup>2</sup>,</span>
            <span class="author-block"><a href="https://sherryy.github.io/">Sherry Yang</a><sup>1,3</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>NYU</span>
            <span class="author-block"><sup>2</sup>UC Berkeley</span>
            <span class="author-block"><sup>3</sup>Google DeepMind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/world-gymnast/world-gymnast"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image hero-figure">
        <img src="./static/images/main_figure.png" alt="World-Gymnast overview">
      </figure>
      <p class="subtitle has-text-justified">
        The policy is trained on tasks specified by an initial frame and language instruction. During training, the policy
        outputs actions which are then passed to the world model which generates imagined rollouts. These rollouts are then
        passed to a VLM which returns a binary task completion reward. This reward is used to update the policy. Once trained,
        we evaluate the policy on real robots. The resulting real world rollouts (frame-action sequences) can be further used
        to improve the world model on the particular environment.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction.
        The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a
        software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation.
        With the recent emergence of world models learned from real-world video-action data, we ask the question of whether
        training a policy in a world model can be more effective than supervised learning or software simulation in achieving
        better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA)
        policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a
        vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms
        software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a
        world model, including training on diverse language instructions and novel scenes from the world model, test-time
        training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world
        model and training robot policies in the cloud could be the key to bridging the gap between robots that work in
        demonstrations and robots that can work in anyone's household.
      </p>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Key Contributions</h2>
    <div class="content">
      <ul>
        <li>Introduce World-Gymnast, an RL framework that fine-tunes VLA policies inside a learned video world model with VLM-based rewards.</li>
        <li>Demonstrate improved real-robot performance over SFT and simulator-based RL.</li>
        <li>Show that training with distractor augmentation, novel language instructions, and additional tasks improves robustness and success.</li>
        <li>Demonstrate test-time training from a novel frame and iterative world model + policy improvement via a Dyna-style loop.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Experiments</h2>
    <div class="content">
      <p>
        World-Gymnast is evaluated on real-robot tasks in the Bridge setup. Below are the reported success rates.
      </p>
      <h3 class="title is-5">Real-robot evaluation: Simulator RL vs World-Gymnast</h3>
      <figure class="image figure-panel">
        <img src="./static/images/sim.png" alt="Real-robot evaluation: SIMPLER vs World-Gymnast results">
      </figure>

      <h3 class="title is-5">Supervised Learning vs World-Gymnast</h3>
      <figure class="image figure-panel">
        <img src="./static/images/sft.png" alt="Real-robot evaluation: SFT vs Iter-SFT vs World-Gymnast results">
      </figure>
      <div class="content">
        <h3 class="title is-5">Real-robot: Put eggplant into blue sink</h3>
        <div class="columns is-multiline is-variable is-4">
          <div class="column is-one-third">
            <div class="video-card">
              <video autoplay muted loop controls playsinline preload="metadata">
                <source src="./static/videos/rl_eggplant.mp4" type="video/mp4">
              </video>
              <p class="video-caption">World-Gymnast (RL)</p>
            </div>
          </div>
          <div class="column is-one-third">
            <div class="video-card">
              <video autoplay muted loop controls playsinline preload="metadata">
                <source src="./static/videos/simpler_eggplant.mp4" type="video/mp4">
              </video>
              <p class="video-caption">SIMPLER (simulator RL)</p>
            </div>
          </div>
          <div class="column is-one-third">
            <div class="video-card">
              <video autoplay muted loop controls playsinline preload="metadata">
                <source src="./static/videos/ctrl_world_eggplant.mp4" type="video/mp4">
              </video>
              <p class="video-caption">Iter-SFT</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Diverse Training Settings</h2>
    <div class="content">
      <p>
        World-Gymnast supports additional training data through distractor augmentation, novel language instructions, and scaling
        the number of tasks. Reported success rates on the OpenVLA held-out split:
      </p>
      <table class="table is-fullwidth is-striped">
        <thead>
          <tr>
            <th>Variant</th>
            <th>Success Rate</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>SFT</td>
            <td>58 ± 4%</td>
          </tr>
          <tr>
            <td>World-Gymnast</td>
            <td>74 ± 3%</td>
          </tr>
          <tr>
            <td>World-Gymnast-Distract</td>
            <td>78 ± 2%</td>
          </tr>
          <tr>
            <td>World-Gymnast-Language</td>
            <td>81 ± 1%</td>
          </tr>
          <tr>
            <td>World-Gymnast-Scaled</td>
            <td>81 ± 4%</td>
          </tr>
        </tbody>
      </table>
      <div class="columns is-multiline is-variable is-4">
        <div class="column is-one-third">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/distractor.mp4" type="video/mp4">
            </video>
            <p class="video-caption">Distractor training: put yellow corn on pink plate</p>
          </div>
        </div>
        <div class="column is-one-third">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/oodlang.mp4" type="video/mp4">
            </video>
            <p class="video-caption">Novel language: put plate on drying rack</p>
          </div>
        </div>
        <div class="column is-one-third">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/scaled.mp4" type="video/mp4">
            </video>
            <p class="video-caption">Scaled tasks: close fridge</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Distractor Robustness: Lift AAA Battery</h2>
    <div class="content">
      <p>
        Task: Lift AAA Battery. Both SFT and World-Gymnast are distracted and pick up the rubber duck, while
        World-Gymnast-Distract completes the task.
      </p>
      <div class="columns is-multiline is-variable is-4">
        <div class="column is-one-third">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/sftdist.mp4" type="video/mp4">
            </video>
            <p class="video-caption">SFT (distracted)</p>
          </div>
        </div>
        <div class="column is-one-third">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/rldist.mp4" type="video/mp4">
            </video>
            <p class="video-caption">World-Gymnast (distracted)</p>
          </div>
        </div>
        <div class="column is-one-third">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/distdist.mp4" type="video/mp4">
            </video>
            <p class="video-caption">World-Gymnast-Distract (success)</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Test-Time Training & Iterative Updates</h2>
    <div class="content">
      <p>
        World-Gymnast can perform test-time RL from a novel frame without real-world rollouts, improving the close-the-drawer
        task from 62 ± 6% to 100 ± 0%. The framework also supports Dyna-style iterative updates: real-robot rollouts are used to
        refine the world model, which then yields higher-quality imagined rollouts and further policy improvements.
      </p>
      <div class="columns is-multiline is-variable is-4">
        <div class="column is-half">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/ttt_autoeval.mp4" type="video/mp4">
            </video>
            <p class="video-caption">Test-time training policy (real world)</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/dyna_autoeval.mp4" type="video/mp4">
            </video>
            <p class="video-caption">Iteratively improved policy (real world)</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">World Model Rollout Fidelity</h2>
    <div class="content">
      <p>
        Comparison of rolling out the same action sequence in different environments.
      </p>
      <div class="columns is-multiline is-variable is-4">
        <div class="column is-half">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/real.mp4" type="video/mp4">
            </video>
            <p class="video-caption">Real robot</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/simpler.mp4" type="video/mp4">
            </video>
            <p class="video-caption">SIMPLER (software simulator)</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/worldgym.mp4" type="video/mp4">
            </video>
            <p class="video-caption">WorldGym (pretrained world model)</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="video-card">
            <video autoplay muted loop controls playsinline preload="metadata">
              <source src="./static/videos/dyna.mp4" type="video/mp4">
            </video>
            <p class="video-caption">World-Gymnast with online updates</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
        under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
