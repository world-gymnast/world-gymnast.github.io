<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="World-Gymnast trains robot policies with reinforcement learning inside a learned world model, using VLM-based rewards.">
  <meta name="keywords" content="robot learning, reinforcement learning, world models, VLA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>World-Gymnast: Training Robots with Reinforcement Learning in a World Model</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">World-Gymnast: Training Robots with Reinforcement Learning in a World Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Ansh Kumar Sharma<sup>1</sup>,</span>
            <span class="author-block">Yixiang Sun<sup>1</sup>,</span>
            <span class="author-block">Ninghao Lu<sup>1</sup>,</span>
            <span class="author-block">Yunzhe Zhang<sup>1</sup>,</span>
            <span class="author-block">Jiarao Liu<sup>2</sup>,</span>
            <span class="author-block">Sherry Yang<sup>1,3</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>NYU</span>
            <span class="author-block"><sup>2</sup>UC Berkeley</span>
            <span class="author-block"><sup>3</sup>Google DeepMind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/world-gymnast/world-gymnast"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image hero-figure">
        <img src="./static/images/main_figure.png" alt="World-Gymnast overview">
      </figure>
      <p class="subtitle has-text-centered">
        World-Gymnast fine-tunes vision-language-action policies by rolling out actions in a learned world model and scoring
        trajectories with a vision-language model.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Robot learning from interacting with the physical world is bottlenecked by the cost of physical interaction. Two
        alternatives—supervised fine-tuning from expert demonstrations and reinforcement learning in software simulation—are
        limited by data availability and the sim-to-real gap. With the emergence of world models learned from real-world
        video-action data, we ask whether training a policy in a world model can outperform supervised learning or simulator-based
        RL on real robots. We propose World-Gymnast, which performs RL fine-tuning of a vision-language-action policy by rolling
        out the policy in an action-conditioned video world model and rewarding rollouts with a vision-language model. On the
        Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulation by as much as 2x.
        World-Gymnast also enables training on diverse language instructions and novel scenes, test-time training from a novel
        frame, and iterative world model and policy improvement.
      </p>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Key Contributions</h2>
    <div class="content">
      <ul>
        <li>Introduce World-Gymnast, an RL framework that fine-tunes VLA policies inside a learned video world model with VLM-based rewards.</li>
        <li>Demonstrate improved real-robot performance over SFT and simulator-based RL on Bridge/AutoEval tasks.</li>
        <li>Show that training from any frame, novel language, and distractor-augmented scenes improves generalization.</li>
        <li>Enable test-time training and iterative world-model/policy updates via a Dyna-style loop.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Method</h2>
    <div class="content">
      <p>
        World-Gymnast trains a policy by rolling out trajectories inside a learned world model (WorldGym). A vision-language
        model assigns a binary success reward for each rollout, and Group Relative Policy Optimization (GRPO) updates the policy
        using group-normalized advantages.
      </p>
      <div class="notification is-light">
        Method diagram coming soon. Share the pipeline figure and I will place it here.
      </div>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Experiments</h2>
    <div class="content">
      <p>
        World-Gymnast is evaluated on the Bridge robot setup and AutoEval real-robot tasks. Below are the reported success rates.
      </p>
      <h3 class="title is-5">AutoEval: Simulator RL vs World-Gymnast</h3>
      <table class="table is-fullwidth is-striped">
        <thead>
          <tr>
            <th>Task</th>
            <th>SIMPLER</th>
            <th>World-Gymnast</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Open the drawer</td>
            <td>34 ± 7%</td>
            <td>58 ± 4%</td>
          </tr>
          <tr>
            <td>Close the drawer</td>
            <td>74 ± 5%</td>
            <td>62 ± 6%</td>
          </tr>
          <tr>
            <td>Put eggplant into blue sink</td>
            <td>32 ± 10%</td>
            <td>72 ± 10%</td>
          </tr>
          <tr>
            <td>Put eggplant into yellow basket</td>
            <td>40 ± 10%</td>
            <td>78 ± 2%</td>
          </tr>
        </tbody>
      </table>

      <h3 class="title is-5">Supervised Learning vs World-Gymnast</h3>
      <table class="table is-fullwidth is-striped">
        <thead>
          <tr>
            <th>Task</th>
            <th>SFT</th>
            <th>Iter-SFT</th>
            <th>World-Gymnast</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Open the drawer</td>
            <td>40 ± 10%</td>
            <td>30 ± 10%</td>
            <td>58 ± 4%</td>
          </tr>
          <tr>
            <td>Close the drawer</td>
            <td>62 ± 6%</td>
            <td>60 ± 8%</td>
            <td>62 ± 6%</td>
          </tr>
          <tr>
            <td>Put eggplant into blue sink</td>
            <td>4 ± 4%</td>
            <td>10 ± 3%</td>
            <td>72 ± 10%</td>
          </tr>
          <tr>
            <td>Put eggplant into yellow basket</td>
            <td>8 ± 4%</td>
            <td>17 ± 17%</td>
            <td>78 ± 2%</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Diverse Training Settings</h2>
    <div class="content">
      <p>
        World-Gymnast supports additional training data through distractor augmentation, novel language instructions, and scaling
        the number of tasks. Reported success rates on the OpenVLA held-out split:
      </p>
      <table class="table is-fullwidth is-striped">
        <thead>
          <tr>
            <th>Variant</th>
            <th>Success Rate</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>SFT</td>
            <td>58 ± 4%</td>
          </tr>
          <tr>
            <td>World-Gymnast</td>
            <td>74 ± 3%</td>
          </tr>
          <tr>
            <td>World-Gymnast-Distract</td>
            <td>78 ± 2%</td>
          </tr>
          <tr>
            <td>World-Gymnast-Language</td>
            <td>81 ± 1%</td>
          </tr>
          <tr>
            <td>World-Gymnast-Scaled</td>
            <td>81 ± 4%</td>
          </tr>
        </tbody>
      </table>
      <div class="notification is-light">
        Qualitative rollouts and comparison figures coming soon.
      </div>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Distractor Robustness: Lift AAA Battery</h2>
    <div class="content">
      <p>
        Task: Lift AAA Battery. Both SFT and World-Gymnast are distracted and pick up the rubber duck, while
        World-Gymnast-Distract completes the task.
      </p>
      <div class="columns is-multiline is-variable is-4">
        <div class="column is-one-third">
          <div class="video-card">
            <video controls playsinline preload="metadata">
              <source src="./static/videos/sftdist.mp4" type="video/mp4">
            </video>
            <p class="video-caption">SFT (distracted)</p>
          </div>
        </div>
        <div class="column is-one-third">
          <div class="video-card">
            <video controls playsinline preload="metadata">
              <source src="./static/videos/rldist.mp4" type="video/mp4">
            </video>
            <p class="video-caption">World-Gymnast (distracted)</p>
          </div>
        </div>
        <div class="column is-one-third">
          <div class="video-card">
            <video controls playsinline preload="metadata">
              <source src="./static/videos/distdist.mp4" type="video/mp4">
            </video>
            <p class="video-caption">World-Gymnast-Distract (success)</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Test-Time Training & Iterative Updates</h2>
    <div class="content">
      <p>
        World-Gymnast can perform test-time RL from a novel frame without real-world rollouts, improving the close-the-drawer
        task from 62 ± 6% to 100 ± 0%. The framework also supports Dyna-style iterative updates: real-robot rollouts are used to
        refine the world model, which then yields higher-quality imagined rollouts and further policy improvements.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Figure 3: World Model Rollout Fidelity</h2>
    <div class="content">
      <p>
        Comparison of rolling out the same action sequence in different environments.
      </p>
      <div class="columns is-multiline is-variable is-4">
        <div class="column is-half">
          <div class="video-card">
            <video controls playsinline preload="metadata">
              <source src="./static/videos/real.mp4" type="video/mp4">
            </video>
            <p class="video-caption">Real robot (AutoEval)</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="video-card">
            <video controls playsinline preload="metadata">
              <source src="./static/videos/simpler.mp4" type="video/mp4">
            </video>
            <p class="video-caption">SIMPLER (software simulator)</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="video-card">
            <video controls playsinline preload="metadata">
              <source src="./static/videos/worldgym.mp4" type="video/mp4">
            </video>
            <p class="video-caption">WorldGym (pretrained world model)</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="video-card">
            <video controls playsinline preload="metadata">
              <source src="./static/videos/dyna.mp4" type="video/mp4">
            </video>
            <p class="video-caption">World-Gymnast with online updates</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Limitations</h2>
    <div class="content">
      <p>
        World-Gymnast can struggle when the initial frame is far from the world model's training distribution. It also relies on a
        pretrained VLM for reward signals, which may hallucinate and produce suboptimal training signals. Improving reward models
        and preventing reward hacking remain important future directions.
      </p>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
        under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
